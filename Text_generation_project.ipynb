{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies \n",
    "import numpy \n",
    "import sys \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense , Dropout , LSTM\n",
    "from keras.utils import np_utils \n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "# loading data and opening our input data in form f a txt file \n",
    "# Project Gutenberg is where the data can be found\n",
    "file = open(\"Frankenstein_2.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#standardization \n",
    "#what is tokenization ? Tokenization is the process of breaking a stream of text up into word phrases symbols or other\n",
    "#meaningful elements\n",
    "def tokenize_words(input):\n",
    "    #lowercase everything to standardise it \n",
    "    input = input.lower()\n",
    "    #instantiating the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizing the text into tokens \n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    #filturing the stopwords using lambda\n",
    "    filtered = filter(lambda token : token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "#preprocess the input data make tokens \n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to chars to numbers\n",
    "# convert character in our input to numbers \n",
    "# we'll sort the list of all characters that appear in out i/p text and then use he enumerate fc \n",
    "# to get numbers that represent the characters \n",
    "# we'll then create a dictionary that stores the keys and values, or the characters and the numbers that represent them\n",
    "\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i , c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of characters: 269995\n",
      "Total Vocab: 43\n"
     ]
    }
   ],
   "source": [
    "# check if words to chars or chars to nums has worked ?\n",
    "# just so we can get an idea of whether our process of converting to characters has worked \n",
    "# we print the length of our variables \n",
    "\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total Number of characters:\", input_len)\n",
    "print(\"Total Vocab:\", vocab_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg length \n",
    "# we're defining how long we want an individual sequence here\n",
    "# an individual sequence is a complete mapping of input characters as integers\n",
    "\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns : 269895\n"
     ]
    }
   ],
   "source": [
    "# loops through sequence \n",
    "# here we're going through the entire list of i/p and converting the chars to numbers with a for looop \n",
    "# this will create a bunch of sequence starts with the next character is the i/p data \n",
    "# beginning  with the first character \n",
    "\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # define i/p and o/p sequences \n",
    "    # i/p is the current character plus the desired sequence length \n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    # out sequence is the initial character plus total sequence length \n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    # converting the list of characters to integer based on previous values and appending the values to our lists \n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "# check to see how many total input sequences we have \n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns :\" , n_patterns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input sequence to np array and network can use \n",
    "X = numpy.reshape(x_data , (n_patterns  ,seq_length ,1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one - hot encoding our label data \n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model \n",
    "# creating a sequential model \n",
    "# dropout is used to prevent overfititng \n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1] , X.shape[2]),return_sequences = True))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256 , return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath , monitor ='loss' , verbose =1 ,save_best_only= True,mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "269895/269895 [==============================] - 685s 3ms/step - loss: 2.9067\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.90671, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "269895/269895 [==============================] - 655s 2ms/step - loss: 2.6408\n",
      "\n",
      "Epoch 00002: loss improved from 2.90671 to 2.64079, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "269895/269895 [==============================] - 656s 2ms/step - loss: 2.5068\n",
      "\n",
      "Epoch 00003: loss improved from 2.64079 to 2.50679, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "217600/269895 [=======================>......] - ETA: 2:06 - loss: 2.4160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fit the model and let it train \n",
    "model.fit(X,y,epochs=4, batch_size=256, callbacks = desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompile model with the same weights \n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy' , optimizer ='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the mdoel back into characters\n",
    "num_to_char = dict((i,e) for i , e in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed :\n",
      "\" r chance led place concealment dared blast presence might unfailing aim put end existence monstrous  \"\n"
     ]
    }
   ],
   "source": [
    "# random seed to help ganerate\n",
    "start = numpy.random.randint(0,len(x_data)-1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed :\")\n",
    "print(\"\\\"\",''.join([num_to_char[value] for value in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare sear"
     ]
    }
   ],
   "source": [
    "# generate the text \n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x,verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
